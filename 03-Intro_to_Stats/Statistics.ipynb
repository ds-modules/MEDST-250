{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../share/Aerial_view_LLNL.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 15px ; height: 250px; background-position: 0% 80%'>\n",
    "    <div style=\"float: center ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.8) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.9) ; line-height: 100%\">Notebook 5:</div>\n",
    "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Statistics for Digital Humanists</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.7)\">Estimated Time: 60 minutes</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "In this jupyter notebook, we will learn some basic statistics, cover the basics of linear regression, and, finally, examine word counts in a text:\n",
    "\n",
    "1. [Mean, Median, Mode](#1)\n",
    "2. [Standard Deviation](#2)\n",
    "3. [T-test](#3)\n",
    "4. [Correlation](#4)\n",
    "5. [Intro to Regression](#5)\n",
    "6. [Frequenices of Words](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparing Our Dataset  <a id='0'></a>\n",
    "We will use some metadata on manuscripts.\n",
    "\n",
    "For the most part, you don't have to worry about all the codes we used in this section to fix the data and make it more usable for out purpose of learning basic statistic. Instead, you can skip to the end of this section 0 and try to familiarize yourself with the [cleaned dataset](#clean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This simply sets up the notebook. Don't worry about this for now.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/BSB-HS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(x):\n",
    "    \n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    \n",
    "    pattern0 = r'([0-9]+)\\./([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern0, x) != None:\n",
    "        tot = int(re.search(pattern0, x).groups()[0] + '00') + int(re.search(pattern0, x).groups()[1] + '00')\n",
    "        return tot/2\n",
    "\n",
    "    pattern3 = r'1\\.\\sHälfte\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern3, x) != None:\n",
    "        return int(re.search(pattern3, x).groups()[0] + '00') - 75\n",
    "    \n",
    "    pattern4 = r'2\\.\\sHälfte\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern4, x) != None:\n",
    "        return int(re.search(pattern4, x).groups()[0] + '00') - 25\n",
    "    \n",
    "    pattern5 = r'Mitte\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern5, x) != None:\n",
    "        return int(re.search(pattern5, x).groups()[0] + '00') - 50\n",
    "\n",
    "    pattern6 = r'Ende\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern6, x) != None:\n",
    "        return int(re.search(pattern6, x).groups()[0] + '00') - 10\n",
    "    \n",
    "    pattern7 = r'1\\.\\sViertel\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern7, x) != None:\n",
    "        return int(re.search(pattern7, x).groups()[0] + '00') - 80\n",
    "    \n",
    "    pattern8 = r'2\\.\\sViertel\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern8, x) != None:\n",
    "        return int(re.search(pattern8, x).groups()[0] + '00') - 60\n",
    "    \n",
    "    pattern9 = r'3\\.\\sViertel\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern9, x) != None:\n",
    "        return int(re.search(pattern9, x).groups()[0] + '00') - 40\n",
    "    \n",
    "    pattern10 = r'4\\.\\sViertel\\s([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern10, x) != None:\n",
    "        return int(re.search(pattern10, x).groups()[0] + '00') - 20\n",
    "    \n",
    "    pattern1 = r'([0-9]+)\\.\\sJh\\.'\n",
    "    if re.search(pattern1, x) != None:\n",
    "        return int(re.search(pattern1, x).groups()[0] + '00') - 100\n",
    "    \n",
    "    pattern2 = r'([0-9]{4})'\n",
    "    if re.search(pattern2, x) != None:\n",
    "        return int(re.search(pattern2, x).groups()[0])\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_extent(x):\n",
    "    \n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    \n",
    "    pattern = r'([0-9]+)\\sBl\\.'\n",
    "    if re.search(pattern, x) != None:\n",
    "        return int(re.search(pattern, x).groups()[0])\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def fix_lang(x):\n",
    "    if type(x) == str and len(x) > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df['Creation'].apply(get_year)\n",
    "df['Pages'] = df['Extent'].apply(get_extent)\n",
    "df['Area'] = df['Height'] * df['Width']\n",
    "df['Language'] = df['Language'].apply(fix_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Dataset: <a id='clean'></a>\n",
    "\n",
    "From the cleaned version of the dataset, we will isolate the columns we will use and create a new table called `manuscripts.` We will be working with this to learn basic statistics. Take a look at the first 5 rows of our dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts = df[['Year', 'Pages', 'Height', 'Width', 'Area','Language']]\n",
    "manuscripts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean, Median, Mode  <a id='1'></a>\n",
    "\n",
    "Let's review the definitions of the three main measures of central tendency: **Mean, Median, Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean\n",
    "\n",
    "The mean of a set of values is the quantity given by the following formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Sum of values}}{\\text{# of values}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to figure out the average number of pages from all the manuscripts in our dataset. To find this, we first isolate the `Pages` coumn and apply the `mean()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "manuscripts['Pages'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "The \"middle\" value of a dataset. That is, if you make an ordered list of the values from smallest to largest, the median is the middle value. If the number of values is even, then the median is the average of the 2 middle values. Similar to `mean`, there is a `median()` function that you can call on a set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts['Pages'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode\n",
    "\n",
    "The mode of a set of values is a value that occurs the most frequently. There can be multiple modes in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts['Pages'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick exercise, find the mean, median, and mode of the `Year` column of the manuscripts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ellipses with your own code\n",
    "year_mean = ...\n",
    "year_median = ...\n",
    "year_mode = ...\n",
    "\n",
    "print(\"The mean of the Year column is: \", year_mean)\n",
    "print(\"The median of the Year column is: \", year_median)\n",
    "print(\"The mode of the Year column is: \", year_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! But that's too easy. And it really doesn't give much insight. What if we want to find out the average page of manuscripts in each of the different languages?\n",
    "\n",
    "First we isolate the two columns we need: `Language` and `Pages`. Then we apply the `dropna()` function that removes the rows with missing values. Then, we use the `groupby()` function and put `Language` as a parameter, indicating we are \"grouping\" by the `Language` column. Finally, we select the `Pages` column and again use the `mean()` function to calculate the average *by Language*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts[['Language', 'Pages']].dropna().groupby(['Language'])['Pages'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now, can you try figuring out how to get the average `Area` by `Language`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now that we know how to do some basic calculations, we can do more interesting things: data visualization! One of the most useful visualization for statistics is called a **boxplot**. Boxplots are a standardized way of displaying the distribution of data:\n",
    "![a](https://pro.arcgis.com/en/pro-app/help/analysis/geoprocessing/charts/GUID-0E2C3730-C535-40CD-8152-80D794A996A7-web.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a [boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) of the number of pages of each manuscript by langauge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(11.7, 8.27))\n",
    "ax = sns.boxplot(ax = ax, data=manuscripts.dropna(),x='Pages', y='Language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to study the boxplot and lcoate the minimum, median, and maximum number of pages by language. You can also start to compare the average number of pages of manuscripts found in different languages by looking at where the medians are. What language has, on average, the most number of pages per manuscripts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another one. Here's a boxplot of the height of each manuscript by langauge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11.7, 8.27))\n",
    "ax = sns.boxplot(ax = ax, data=manuscripts.dropna(),x='Height', y='Language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn! Try creating a boxplot of the `Width` of the manuscripts by `Language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a box plot of the `Height` of the manuscripts by `Language`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Boxplots are powerful visualizations that can give a quick overview of that data's basic make up, making it also useful for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Deviation (SD)  <a id='2'></a>\n",
    "\n",
    "Standard deviation (SD) \"measures roughly how far the numbers on the list are from their average\" by calculating the root mean square of deviations from average:\n",
    "\n",
    "$$\n",
    "=\\sqrt{\\frac{\\text{Sum[(value$_{1}$ - average)$^2$ + (value$_{2}$ - average)$^2$ ... (value$_{n}$ - average)$^2$]}}{n}}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the SD of a number we simply use the `np.std()`. For example, to find out the SD of the number of pages of all the greek manuscripts, we will use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_manuscipts = manuscripts.loc[df['Language'] == 'Greek'].dropna()\n",
    "np.std(greek_manuscipts[\"Pages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can infer from this is that the average Greek manuscript's page length is roughly 121 pages more or less than the mean of all page lengths of all Greek manuscripts, which is 161 pages. \n",
    "\n",
    "Below, we can see all the SDs for each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(greek_manuscipts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T-Test  <a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[T-tests](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) are a method of testing hypotheses in statistics. They are used to help determine if there is a significant likelihood that the means of two independent populations differ.\n",
    "\n",
    "To illustrate this concept, let's first come up with a research question. Suppose we wanted to know if there exists a significant difference in average page length between Latin and German manuscripts.\n",
    "\n",
    "**Our null hypothesis**: There is no substantial difference in average page length between Latin and German manuscripts in general. That is, the difference between average page lengths is 0. \n",
    "\n",
    "Using our samples of Latin and German manuscripts, we can proceed to use a T-test to test this hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first select the page lengths for all German manuscripts in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = manuscripts[manuscripts['Language']=='German'][['Language', 'Pages']].dropna()\n",
    "group1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and all Latin manuscripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2 = manuscripts[manuscripts['Language']=='Latin'][['Language', 'Pages']].dropna()\n",
    "group2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two different, independent samples, we can use the function `ttest_ind`, which takes in 2 arrays, one for each group, to perform the T-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(group1['Pages'], group2['Pages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **p-value** is less than .005.\n",
    "\n",
    "The p-value is essentially the probability that the result we observed happened by chance. Thus, the lower the p-value, the higher the likelihood that the mean between Latin and German manuscripts in general differs from 0. \n",
    "\n",
    "In addition, we say we \"reject the null hypothesis\" when the p-value is below a certain threshold. \n",
    "\n",
    "> If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, then we reject the null hypothesis of equal averages.\n",
    "\n",
    "So, in this case, we can reject the null hypothesis at the 1% (p-value less than 0.01) threshold, and we can be confident that there is indeed a difference in average page length between Latin and German manuscripts in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation <a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation measures the strength of a linear association between two variables. We want to know: what happens to one variable as the other increases? Is there an obvious linear pattern? \n",
    "\n",
    "* If one variable increases when the other increases, the two variables are **positively correlated**. \n",
    "* Conversely, when one variable increases when the other decreases, the two variables are **negatively correlated**.\n",
    "\n",
    "\n",
    "\n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect Positive Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dummy data\n",
    "pos_x = np.arange(0, 50)\n",
    "pos_y = 3*pos_x + 5\n",
    "sns.regplot(x=pos_x, y=pos_y, color=\"blue\")\n",
    "plt.xlim(0, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect Negative Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_x = np.arange(0, 50)\n",
    "neg_y = -4*neg_x + 5\n",
    "sns.regplot(x=neg_x, y=neg_y, color=\"red\")\n",
    "plt.xlim(0, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, perfect correlation implies that the relationship between the two variables can be defined by a line. However, perfect correlation rarely occurs in the real world. It's more likely that you will see data such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"Height\", y=\"Width\", data=manuscripts, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it isn't perfect correlation, the variables Height and Width do seem to be strongly positively correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"Year\", y=\"Pages\", data=manuscripts, color = \"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the Year and Pages variables show little to no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between two variables can be quantified by $r$, the **correlation coefficient**, where\n",
    "\n",
    "$$\n",
    "-1 \\leq r \\leq 1\n",
    "$$\n",
    "\n",
    "That is, $r$ ranges from -1 to 1, depending on the direction and strength of correlation. \n",
    "\n",
    "> **Direction**: The sign of $r$ denotes where the correlation is positive or negative. The coefficient is 1 when the two variables are perfectly positively correlated and -1 when the two variables are perfectly negatively correlated.\n",
    "\n",
    "> **Strength**: The closer $r$ is to 0, the less the two variables are correlated. In other words, the greater the absolute value of $r$, the greater the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask, what is the correlation between Height and Width?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one command, we can create a correlation matrix, showing the correlation between each variable in our table with every other variable. Notice that each variable is perfectly correlated with itself, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manuscripts.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick exercise, take a look at the table and find a pair variables that:\n",
    "* positively correlated\n",
    "* negatively correlated\n",
    "* strongly correlated (positive or negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Intro to Regression  <a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover the basics of Linear Regression, a method in the social sciences that is used to determine the effect of an **independent variable** on a **dependent variable**. In other words, we want to find out whether a change in an independent variable changes the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Say we want to find out the effect of a manuscript's width on its area. In this case, the width is the independent variable and the area is the dependent variable. Then, we run a regression where the dependent variable is \"regressed on\" the independent variable. In this case, Area will be regressed on Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library we will be using is called `statsmodels`. First, let's specify the regression formula/model. This is given in the form:\n",
    "\n",
    "$$\n",
    "\\text{formula = dependent variable ~ independent variable}\n",
    "$$\n",
    "\n",
    "Since we are regressing Area on Width, our formula will be $\\text{Area ~ Width} $, where $\\text{Area}$ and $\\text{Width}$ are columns in our `manuscripts` table from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula='Area ~ Width', data=manuscripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you enter the formula as a string. \n",
    "\n",
    "Now, let's \"fit\" our model to the data. What this code does is that, if we plotted Area and Width on a graph, it will fit a line through the points, similar to what we saw earlier in the correlation section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the regression summary, which will give us the information we were looking for: What is the effect of Width on the Area of a manuscript? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information in the regression summary, but don't get overwhelmed. We will only be focusing on a couple details for our purposes today.\n",
    "\n",
    "* Look at the **coef** column in the middle panel from the top. Look at the value in Width row. This is roughly the average change in area for a one-unit change in Width.\n",
    "\n",
    "* Also in the Width row in the middle panel, look at the column labeled **P > |t|**. Recall the T-tests from earlier. This is the p-value of a T-test performed on the coefficient. What you need to know is that, if the p-value is below a certain threshold, say .05 or .01, then you can reject the null hypothesis that the Width of a manuscript is not associated with its Area.\n",
    "\n",
    "Please ask the lab assistant(s) for help if you are having trouble with these concepts. They are not always easy to pick up the first time you are introduced to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find two (numerical) columns, one dependent variable and one independent variable, from the `manuscripts` table and perform a regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the ellipses with your regression formula\n",
    "mod = smf.ols(formula= ..., data=manuscripts)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now interpret the results based on what we discussed earlier. Edit the markdown cell immediately below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a Markdown cell. Discuss your findings here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Frequenices of Words <a id='6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we will look at how to calculate and plot word frequencies in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read in our example text file `Anon_Gawain.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Anon_Gawain.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split the text file into pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[pP]age\\s+[0-9]+')\n",
    "pages = re.split(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many pages are there in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split each page into its words using `split()`. \n",
    "\n",
    "Also, we can create a dictionary `words_by_page`, so that we can look up the words in a page by page number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_page = {}\n",
    "page_number = 1\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for page in pages:\n",
    "    split_page = page.split()\n",
    "    # Adds the list of words to our dictionary\n",
    "    words_by_page[page_number] = split_page\n",
    "    page_number += 1\n",
    "    all_words.extend(split_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the total number of words in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of all the words on a certain page, index the `words_by_page` dictionary with square brackets [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words on page 8\n",
    "words_by_page[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the frequencies of words within pages and between pages.\n",
    "\n",
    "`Counter` is a function that, as the name suggests, counts the number of appearances of each word in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Yields the 10 most common words in page 8\n",
    "Counter(words_by_page[8]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, write code below that yields the 8 most common words on page 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up how many times a certain word appears in a page, using `Counter`. Say we wanted to know how many times \"þis\" appears on page 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(words_by_page[34])[\"þis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that gives us a DataFrame containing the frequencies per page of a given word. This way, we can compare frequencies directly between pages directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(word):\n",
    "    \"\"\"Returns a DataFrame containing the frequency counts \n",
    "    per page of the given word\"\"\"\n",
    "    \n",
    "    word_df = pd.DataFrame()\n",
    "    word_df[\"Page\"] = np.arange(1, len(pages)+1)\n",
    "    \n",
    "    word_counts = []\n",
    "    \n",
    "    # Goes page by page, calculating the counts of the word\n",
    "    for page_num in word_df[\"Page\"]:\n",
    "        count = Counter(words_by_page[page_num])[word]\n",
    "        word_counts.append(count)\n",
    "    \n",
    "    word_df[\"Frequency\"] = np.array(word_counts)\n",
    "    return word_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `word_counts` to see how many times \"þis\" appears on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts(\"þis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
